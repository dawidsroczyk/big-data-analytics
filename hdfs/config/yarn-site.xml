<?xml version="1.0"?>
<configuration>

    <!-- â­ NajwaÅ¼niejsze: ResourceManager -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>resourcemanager</value>
        <!--
            Nazwa hosta musi byÄ‡ taka jak kontener w docker-compose.
        -->
    </property>

    <!-- ðŸ“¡ Porty RM -->
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>resourcemanager:8032</value>
    </property>

    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>resourcemanager:8030</value>
    </property>

    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>resourcemanager:8031</value>
    </property>

    <!-- ðŸŽ› UI ResourceManager -->
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>resourcemanager:8088</value>
    </property>

    <!-- â­ NodeManager podstawowe -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        <!--
            Wymagane dla Spark + MapReduce shuffle.
        -->
    </property>

    <!-- ðŸ›‘ Bardzo waÅ¼ne w Dockerze: wyÅ‚Ä…czamy memory-checki -->
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
        <!--
            Docker != realny system memory cgroup â†’ YARN zabija wszystkie kontenery.
            WyÅ‚Ä…czamy, bo inaczej Spark Streaming padnie po kilku minutach.
        -->
    </property>

    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
        <!--
            To samo dla fizycznej pamiÄ™ci.
        -->
    </property>

    <!-- ðŸ’¾ DomyÅ›lne limity pamiÄ™ci kontenerÃ³w -->
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>4096</value>  <!-- 4 GB  na kadego -->
    </property>

    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>256</value>
    </property>

    <!-- âš™ DomyÅ›lne CPU dla kontenerÃ³w -->
    <property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>1</value>
    </property>

    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>2</value>
    </property>

    <!-- â­ Bardzo waÅ¼ne dla Spark Streaming: nie ubijaj aplikacji po pauzie -->
    <property>
        <name>yarn.resourcemanager.am.max-attempts</name>
        <value>10</value>
        <!--
            Spark Streaming AM moÅ¼e siÄ™ restartowaÄ‡ przy restarcie kafki.
            10 prÃ³b = demo nie padnie.
        -->
    </property>

    <!-- ðŸ”„ Restart NM w Dockerze â†’ nie gubimy statusu aplikacji -->
    <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
    </property>

    <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore</value>
    </property>

    <property>
        <name>yarn.resourcemanager.fs.state-store.uri</name>
        <value>hdfs://namenode:8020/rmstate</value>
    </property>

    <!-- ðŸ©¹ NodeManager: opÃ³Åºnienie usuwania logÃ³w (debugging w demo) -->
    <property>
        <name>yarn.nodemanager.delete.debug-delay-sec</name>
        <value>600</value>
        <!--
            10 minut na podejrzenie logÃ³w â€” super przy demo i nauce.
        -->
    </property>

    <!-- ðŸš€ NajwaÅ¼niejsze do streamingu: 
         Spark potrzebuje duÅ¼ych buforÃ³w shuffle -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>4092</value> <!-- 1gb = 1024-->
    </property>

    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>2</value>
    </property>

</configuration>
